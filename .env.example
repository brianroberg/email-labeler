# API Proxy Configuration
#
# If running as part of agent-stack, symlink to the shared .env instead:
#   ln -s ../agent-stack/.env .env
#
# Otherwise, copy this file to .env and fill in your values:
#   cp .env.example .env

# API key for authenticating with the proxy server
PROXY_API_KEY=aproxy_your_api_key_here

# URL of the proxy server (defaults to Docker host)
# PROXY_URL=http://host.docker.internal:8000

# Cloud LLM Configuration (any OpenAI-compatible chat completion endpoint)
# The model name is configured in config.toml, not here.
CLOUD_LLM_URL=https://your-llm-provider.com/v1/chat/completions
CLOUD_LLM_API_KEY=your_api_key_here

# Local LLM Configuration (MLX/Qwen3 via Tailscale)
MLX_URL=http://macbook:8080/v1/chat/completions

# User identity for prompt personalization
# Referenced in config.toml as {env.USER_NAME}
USER_NAME=Your Name
