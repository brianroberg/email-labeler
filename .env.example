# API Proxy Configuration
# Copy this file to .env and fill in your values

# API key for authenticating with the proxy server
PROXY_API_KEY=aproxy_your_api_key_here

# URL of the proxy server (defaults to Docker host)
# PROXY_URL=http://host.docker.internal:8000

# Cloud LLM Configuration (any OpenAI-compatible chat completion endpoint)
# The model name is configured in config.toml, not here.
CLOUD_LLM_URL=https://your-llm-provider.com/v1/chat/completions
CLOUD_LLM_API_KEY=your_api_key_here

# Local LLM Configuration (MLX/Qwen3 via Tailscale)
MLX_URL=http://macbook:8080/v1/chat/completions

# User identity for prompt personalization
# Referenced in config.toml as {env.USER_NAME}
USER_NAME=Your Name
